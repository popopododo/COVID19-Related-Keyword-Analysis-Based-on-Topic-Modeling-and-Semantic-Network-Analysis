{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LiKPMKOSm2U8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\kdw27\\appdata\\roaming\\python\\python36\\site-packages (from tqdm) (0.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar\n",
      "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
      "Building wheels for collected packages: progressbar\n",
      "  Building wheel for progressbar (setup.py): started\n",
      "  Building wheel for progressbar (setup.py): finished with status 'done'\n",
      "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12074 sha256=44961d73edc86707adee1c9e5e52b84ee87fd79a3f5b8ebf7d5e15c987f32c6a\n",
      "  Stored in directory: c:\\users\\kdw27\\appdata\\local\\pip\\cache\\wheels\\bc\\13\\93\\a9bf6b3d3966e4af014b0dbef027fdea47393faf47e990349f\n",
      "Successfully built progressbar\n",
      "Installing collected packages: progressbar\n",
      "Successfully installed progressbar-2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\kdw27\\appdata\\local\\programs\\python\\python36\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyL2yrZAm2VE",
    "outputId": "3313f418-e3ff-40f9-f030-32a058e1d2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본문 크롤링에 필요한 함수를 로딩하고 있습니다...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "검색할 언론사  : 한겨례\n",
      "검색할 키워드  : 백신\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pickle, progressbar, json, glob, time\n",
    "from tqdm import tqdm\n",
    "\n",
    "###### Saving date ##########\n",
    "date = str(datetime.now())\n",
    "date = date[:date.rfind(':')].replace(' ', '_')\n",
    "date = date.replace(':','시') + '분'\n",
    "\n",
    "sleep_sec = 0.5\n",
    "\n",
    "\n",
    "####### 언론사별 본문 위치 태그 파싱 함수 ###########\n",
    "print('본문 크롤링에 필요한 함수를 로딩하고 있습니다...\\n' + '-' * 100)\n",
    "def crawling_main_text(url):\n",
    "    req = requests.get(url)\n",
    "    req.encoding = None\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    # print(url)\n",
    "    \n",
    "    # 연합뉴스\n",
    "    if ('yna.kr' in url):\n",
    "        main_article = soup.find('article', {'class':'story-news article'})\n",
    "        if main_article == None:\n",
    "            main_article = soup.find('div', {'class' : 'article-txt'})                  \n",
    "        text = main_article.text\n",
    "        \n",
    "    # MBC \n",
    "    elif '//imnews.imbc' in url: \n",
    "        text = soup.find('div', {'itemprop' : 'articleBody'}).text\n",
    "        \n",
    "    # 매일경제(미라클), req.encoding = None 설정 필요\n",
    "    elif 'mirakle.mk' in url:\n",
    "        text = soup.find('div', {'class' : 'view_txt'}).text\n",
    "        \n",
    "    # 매일경제, req.encoding = None 설정 필요\n",
    "    elif 'mk.co' in url:\n",
    "        text = soup.find('div', {'class' : 'art_txt'}).text\n",
    "    # 서울신문    \n",
    "    elif 'seoul.co' in url:\n",
    "        text = soup.find('div', {'class' : 'view_txt'}).text\n",
    "        \n",
    "    # 세계일보 \n",
    "    elif 'segye.com' in url:\n",
    "        text = soup.find('div', {'class' : 'view_txt'}).text\n",
    "        \n",
    "    # 조선일보\n",
    "    elif 'chosun.com' in url:\n",
    "        text = soup.find('div', {'class' : 'view_txt'}).text\n",
    "        \n",
    "    # 동아일보\n",
    "    elif 'donga.com' in url:\n",
    "        text = soup.find('div',{'class' : 'view_txt'}).text\n",
    "        \n",
    "    # SBS\n",
    "    elif 'news.sbs' in url:\n",
    "        text = soup.find('div', {'itemprop' : 'articleBody'}).text\n",
    "    \n",
    "    # KBS\n",
    "    elif 'news.kbs' in url:\n",
    "        text = soup.find('div', {'id' : 'cont_newstext'}).text\n",
    "        \n",
    "    # JTBC\n",
    "    elif 'news.jtbc' in url:\n",
    "        text = soup.find('div', {'class' : 'article_content'}).text\n",
    "        \n",
    "    # 중앙일보\n",
    "    elif ('joongang.co' in url) | ('news.joins' in url):\n",
    "        text = soup.find('div', {'class' : 'article_body fs3'}).text\n",
    "            \n",
    "    # 한겨레\n",
    "    elif 'hani.co' in url:\n",
    "        main_article = soup.find('div', {'class' : 'article-text'})\n",
    "        if main_article == None:\n",
    "            main_article = soup.find('div', {'class' : 'news_text01'})\n",
    "        text = main_article.text\n",
    "        \n",
    "    # 오마이뉴스\n",
    "    elif 'ohmynews' in url:\n",
    "        text = soup.find('div', {'class' : 'at_contents'}).text\n",
    "\n",
    "        \n",
    "        \n",
    "    # ETC\n",
    "    else:\n",
    "        text = None\n",
    "        \n",
    "    return text.replace('\\n','').replace('\\r','').replace('<br>','').replace('\\t','')\n",
    "    \n",
    "    \n",
    "press_nm = input('검색할 언론사  : ')\n",
    "\n",
    "\n",
    "############### 브라우저를 켜고 검색 키워드 입력 ####################\n",
    "query = input('검색할 키워드  : ')\n",
    "news_num = int(input('수집 뉴스의 수(숫자만 입력) : '))\n",
    "\n",
    "print('\\n' + '=' * 100 + '\\n')\n",
    "\n",
    "print('브라우저를 실행시킵니다(자동 제어)\\n')\n",
    "chrome_path = \"C:\\\\Users\\\\user\\Downloads\\chromedriver_win32\\chromedriver.exe\"\n",
    "browser = webdriver.Chrome(chrome_path)\n",
    "\n",
    "news_url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'.format(query)\n",
    "\n",
    "    #https://search.naver.com/search.naver?where=news&query={}\n",
    "browser.get(news_url)\n",
    "time.sleep(sleep_sec)\n",
    "\n",
    "\n",
    "######### 언론사 선택 및 confirm #####################\n",
    "print('설정한 언론사를 선택합니다.\\n')\n",
    "\n",
    "search_opn_btn = browser.find_element_by_xpath('//a[@class=\"btn_option _search_option_open_btn\"]')\n",
    "search_opn_btn.click()\n",
    "time.sleep(sleep_sec)\n",
    "\n",
    "bx_press = browser.find_element_by_xpath('//div[@role=\"listbox\" and @class=\"api_group_option_sort _search_option_detail_wrap\"]//li[@class=\"bx press\"]')\n",
    "\n",
    "# 기준 두번 째(언론사 분류순) 클릭하고 오픈하기\n",
    "press_tablist = bx_press.find_elements_by_xpath('.//div[@role=\"tablist\" and @class=\"option\"]/a')\n",
    "press_tablist[1].click()\n",
    "time.sleep(sleep_sec)\n",
    "\n",
    "# 첫 번째 것(언론사 분류선택)\n",
    "bx_group = bx_press.find_elements_by_xpath('.//div[@class=\"api_select_option type_group _category_select_layer\"]/div[@class=\"select_wrap _root\"]')[0]\n",
    "\n",
    "press_kind_bx = bx_group.find_elements_by_xpath('.//div[@class=\"group_select _list_root\"]')[0]\n",
    "press_kind_btn_list = press_kind_bx.find_elements_by_xpath('.//ul[@role=\"tablist\" and @class=\"lst_item _ul\"]/li/a')\n",
    "\n",
    "\n",
    "for press_kind_btn in press_kind_btn_list:\n",
    "    \n",
    "    # 언론사 종류를 순차적으로 클릭(좌측)\n",
    "    press_kind_btn.click()\n",
    "    time.sleep(sleep_sec)\n",
    "    \n",
    "    # 언론사선택(우측)\n",
    "    press_slct_bx = bx_group.find_elements_by_xpath('.//div[@class=\"group_select _list_root\"]')[1]\n",
    "    # 언론사 선택할 수 있는 클릭 버튼\n",
    "    press_slct_btn_list = press_slct_bx.find_elements_by_xpath('.//ul[@role=\"tablist\" and @class=\"lst_item _ul\"]/li/a')\n",
    "    # 언론사 이름들 추출\n",
    "    press_slct_btn_list_nm = [psl.text for psl in press_slct_btn_list]\n",
    "    \n",
    "    # 언론사 이름 : 언론사 클릭 버튼 인 딕셔너리 생성\n",
    "    press_slct_btn_dict = dict(zip(press_slct_btn_list_nm, press_slct_btn_list))\n",
    "    \n",
    "    # 원하는 언론사가 해당 이름 안에 있는 경우\n",
    "    # 1) 클릭하고\n",
    "    # 2) 더이상 언론사분류선택 탐색 중지\n",
    "    if press_nm in press_slct_btn_dict.keys():\n",
    "        print('<{}> 카테고리에서 <{}>를 찾았으므로 탐색을 종료합니다'.format(press_kind_btn.text, press_nm))\n",
    "        \n",
    "        press_slct_btn_dict[press_nm].click()\n",
    "        time.sleep(sleep_sec)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wASSBIsqm2VO",
    "outputId": "f666d1c6-1dbd-4daa-fd52-87d1850c3678",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "크롤링을 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████▎         | 3489/4000 [1:05:57<24:18,  2.85s/it]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bd4d20463ca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mpages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//div[@class=\"sc_page_inner\"]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mnext_page_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.//a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_page_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "################ 뉴스 크롤링 ########################\n",
    "\n",
    "print('\\n크롤링을 시작합니다.')\n",
    "# ####동적 제어로 페이지 넘어가며 크롤링\n",
    "news_dict = {}\n",
    "idx = 1\n",
    "cur_page = 1\n",
    "\n",
    "pbar = tqdm(total=news_num ,leave = True)\n",
    "    \n",
    "while idx < news_num:\n",
    "\n",
    "    table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "    li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n",
    "    area_list = [li.find_element_by_xpath('.//div[@class=\"news_area\"]') for li in li_list]\n",
    "    a_list = [area.find_element_by_xpath('.//a[@class=\"news_tit\"]') for area in area_list]\n",
    "    date_list = [area.find_element_by_xpath('.//span[@class=\"info\"]') for area in area_list]    \n",
    "\n",
    "    for n in a_list[:min(len(a_list), news_num-idx+1)]:\n",
    "        n_url = n.get_attribute('href')\n",
    "        try:\n",
    "            news_dict[idx] = {'title' : n.get_attribute('title'),\n",
    "                            'date' : date_list[(idx-1)%10].text,                                 \n",
    "                            'text' : crawling_main_text(n_url)}\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        idx += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "    if idx < news_num:\n",
    "        cur_page +=1\n",
    "\n",
    "        pages = browser.find_element_by_xpath('//div[@class=\"sc_page_inner\"]')\n",
    "        next_page_url = [p for p in pages.find_elements_by_xpath('.//a') if p.text == str(cur_page)][0].get_attribute('href')\n",
    "\n",
    "        browser.get(next_page_url)\n",
    "        time.sleep(sleep_sec)\n",
    "    \n",
    "    else:\n",
    "        pbar.close()\n",
    "        \n",
    "        print('\\n브라우저를 종료합니다.\\n' + '=' * 100)\n",
    "        time.sleep(0.7)\n",
    "        browser.close()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXSBjRKem2VP"
   },
   "outputs": [],
   "source": [
    "########### 데이터 전처리하기 ##############\n",
    "print('데이터프레임 변환\\n')\n",
    "news_df = DataFrame(news_dict).T\n",
    "\n",
    "folder_path = os.getcwd()\n",
    "xlsx_file_name = '네이버뉴스_{}_{}개_{}_5.xlsx'.format(press_nm, news_num, query)\n",
    "\n",
    "news_df.to_excel(xlsx_file_name)\n",
    "\n",
    "print('엑셀 저장 완료 | 경로 : {}\\\\{}\\n'.format(folder_path, xlsx_file_name))\n",
    "\n",
    "os.startfile(folder_path)\n",
    "\n",
    "print('=' * 100 + '\\n결과물의 일부')\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lfnTGGTm2VQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Crawling_NaverNews.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
